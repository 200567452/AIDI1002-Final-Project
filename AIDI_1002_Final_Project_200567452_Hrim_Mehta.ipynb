{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDI 1002 Final Term Project Report\n",
    "\n",
    "#### Hrim Mehta (200567452)\n",
    "\n",
    "####  Email: 200567452@student.georgianc.on.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction:\n",
    "\n",
    "The paper I am replicating, \"Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting\", proposes a new approach to multivariate timeseries forecasting that first transforms the time-series into block Hankel tensors (BHT) to capture mutual correlations between the multiple timeseries and then applies the ARIMA model to predict future samples. They've evaluated their approach on 3 public datasets (traffic, electricity, and smoke video) and compared their accuracy (RNMSE metric) with competing methods of ARIMA, VAR, XGBOOST, etc. In this project, I will be testing the BHT-ARIMA model on the currency exchange rate dataset and compare its performance with that of ARMAX and LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "| Reference      | Explanation                                                                                                       | Dataset/Input                         | Weakness                                                                                                                                                    |\n",
    "|----------------|-------------------------------------------------------------------------------------------------------------------|---------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Shi et al. [1] | They proposed an approach that performs MDT Hankelization prior to using ARIMA to forecast mutivariate timeseries | Electricity, Traffic, and Smoke video | Tested on 3 public datasets and their model performed best compared to competing models. Need to replicate with other datasets to check if this holds true. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(7588, 8)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/laiguokun/multivariate-time-series-data - source for new dataset to test models on\n",
    "\n",
    "exchange_rate = pd.read_csv(\"datasets/exchange_rate.txt\", sep = ',', header = None)\n",
    "exchange_rate.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = exchange_rate\n",
    "df_diff = df.diff()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "test_size = df_diff.shape[0] - round(df_diff.shape[0] * 90 / 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "train = df[0:-test_size]\n",
    "test = df[-test_size:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model 1: VARMAX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_pickle = pkl.load(open('var_model.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehta\\anaconda3\\envs\\aidi\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:834: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "test_predicted = var_pickle.forecast(steps=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# https://analyticsindiamag.com/complete-guide-to-dickey-fuller-test-in-time-series-analysis/\n",
    "\n",
    "def inverse_diff(actual_df, pred_df):\n",
    "    df_res = pred_df.copy()\n",
    "    columns = actual_df.columns\n",
    "    for col in columns:\n",
    "        df_res[str(col)+'_1st_inv_diff'] = actual_df[col].iloc[-1] + df_res[col].cumsum()\n",
    "    return df_res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "inverted_diff_predicted = inverse_diff(df, test_predicted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VARMAX\n",
      "NRMSE: 2391.615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_true = df.iloc[[round(df_diff.shape[0] * 90 / 100) - 1]] # df[-test_size:]\n",
    "\n",
    "column_to_predict = 2\n",
    "rmse = np.sqrt(mean_squared_error(test_true[column_to_predict], inverted_diff_predicted[str(column_to_predict) + '_1st_inv_diff']))\n",
    "nrmse_mean = rmse / (test_true[column_to_predict].mean())\n",
    "print('Model: VARMAX')\n",
    "print('NRMSE: {:.3f}'.format(nrmse_mean))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model 2: LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('./lstm_model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2020/10/multivariate-multi-step-time-series-forecasting-using-stacked-lstm-sequence-to-sequence-autoencoder-in-tensorflow-2-0-keras/\n",
    "# https://analyticsindiamag.com/how-to-do-multivariate-time-series-forecasting-using-lstm/\n",
    "\n",
    "def split_series(series, n_past, n_future):\n",
    "    X, y = list(), list()\n",
    "    for window_start in range(len(series)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(series):\n",
    "            break\n",
    "        past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
    "        X.append(past)\n",
    "        y.append(future)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_past = 10\n",
    "n_future = 1\n",
    "n_features = df.shape[1]\n",
    "\n",
    "X_test, y_test = split_series(test.values,n_past, n_future)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1],n_features))\n",
    "y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], n_features))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LSTM\n",
      "NRMSE: 0.006\n"
     ]
    }
   ],
   "source": [
    "column_to_predict = 2\n",
    "rmse = np.sqrt(mean_squared_error(y_test[:,0,column_to_predict], predicted[:,0,column_to_predict]))\n",
    "nrmse_mean = rmse / (y_test[:,0,column_to_predict].mean())\n",
    "print('Model: LSTM')\n",
    "print('NRMSE: {:.3f}'.format(nrmse_mean))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model 3: BHT-ARIMA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "ori_ts = np.load('exchange_rate.npy').T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# based on original main.py of BHT-ARIMA - modified for my dataset\n",
    "\n",
    "from BHT_ARIMA import BHTARIMA\n",
    "from BHT_ARIMA.util.utility import get_index\n",
    "\n",
    "ts = ori_ts[..., :-1] # training data,\n",
    "label = ori_ts[..., -1] # label, take the last time step as label\n",
    "p = 2 # p-order\n",
    "d = 1 # d-order\n",
    "q = 2 # q-order\n",
    "taus = [8, 5] # MDT-rank\n",
    "Rs = [5, 5] # tucker decomposition ranks\n",
    "k =  10 # iterations\n",
    "tol = 0.001 # stop criterion\n",
    "Us_mode = 4 # orthogonality mode\n",
    "model = BHTARIMA(ts, p, d, q, taus, Rs, k, tol, verbose=0, Us_mode=Us_mode)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "result, _ = model.run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "predicted = result[..., -1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BHT-ARIMA\n",
      "NRMSE: 37120.933\n"
     ]
    }
   ],
   "source": [
    "nrmse = get_index(predicted, label)['nrmse']\n",
    "print('Model: BHT-ARIMA')\n",
    "print('NRMSE: {:.3f}'.format(nrmse))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion and Future Direction\n",
    "\n",
    "Based on the NRMSE metric, out of the 3 models (VARMAX, LSTM, and BHT-ARIMA) tested in this project for predicting future exchange rates of currencies, LSTM performs the best, followed by VARMAX, and lastly BHT-ARIMA. Both VARMAX and BHT-ARIMA need to be looked into further as I am seeing very high NRMSE values meaning that both of these models are very poor at predicting the currency exchange rates for countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "[1]  Shi, Qiquan, et al. \"Block Hankel tensor ARIMA for multiple short time series forecasting.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n",
    "[1] Shi, Q., Yin, J., Cai, J., Cichocki, A., Yokota, T., Chen, L., ... & Zeng, J. (2020, April). Block Hankel tensor ARIMA for multiple short time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 04, pp. 5758-5766)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
